[["index.html", "Movie Box Office Chapter 1 Proposal 1.1 Data 1.2 Modeling Goal 1.3 Models", " Movie Box Office Hongwei Zha 2023-04-30 Chapter 1 Proposal 1.1 Data The data comes from Kaggle: https://www.kaggle.com/datasets/rajugc/imdb-movies-dataset-based-on-genre. And the original source is IMDB:https://www.imdb.com/interfaces/. For simplicity, I do not intend to use all the data, so I just used data from Kaggle. 1.2 Modeling Goal The main goal would be predicting the box office of a new movie based on its IMDB rating, certificate, genre, run time, year, casts, and directors. Some concerns about the practicability is that casts and directors contain many names, to simplify it, I decide to only include the top three actors and only one main director. Then I will convert them to dummy variables, i.e., each actor or director as a feature. Also note that, what we want to predict is the ultimate gross box office of the movie, so we can collect the new movie’s rating after it is released for certain time. I am also interested in finding whether people prefer old movies or new released movies. For example, I can calculate the mean ratings of movies in each year and see how the trend looks like. 1.3 Models The first model that I intend to use will be the ordinary linear regression model. An issue would be that the underlying relationship is not linear. So I may implement generalized linear model with a Gamma distribution (or I can do some tests to see what kind of exponential distributions that box office is closet to) as the second, decision tree as the third, and random forest as the fourth model. "],["data-pre-processing.html", "Chapter 2 Data pre-processing 2.1 Data Dictionary 2.2 Data Summary 2.3 Box Office Vs Genre 2.4 Box Office Vs Rating &amp; Certificate", " Chapter 2 Data pre-processing 2.1 Data Dictionary movie_id – IMDB Movie ID movie_name – Name of the movie year - Release year certificate – Certificate of the movie run_time – Total movie run time genre – Genre of the movie rating – Rating of the movie description – Description of the movie director – Director of the movie director_id – IMDB id of the director star – Star of the movie star_id – IMDB id of the star votes – Number of votes in IMDB website gross – Gross Box Office of the movie in million 2.2 Data Summary 2.2.1 Data Statistics ## &#39;data.frame&#39;: 318269 obs. of 8 variables: ## $ year : chr &quot;2022&quot; &quot;2022&quot; &quot;2023&quot; &quot;2022&quot; ... ## $ certificate: chr &quot;PG-13&quot; &quot;PG-13&quot; &quot;R&quot; &quot;R&quot; ... ## $ runtime : chr &quot;161 min&quot; &quot;192 min&quot; &quot;107 min&quot; &quot;139 min&quot; ... ## $ genre : chr &quot;Action, Adventure, Drama&quot; &quot;Action, Adventure, Fantasy&quot; &quot;Action, Thriller&quot; &quot;Action, Adventure, Comedy&quot; ... ## $ rating : chr &quot;6,9&quot; &quot;7,8&quot; &quot;6,5&quot; &quot;8&quot; ... ## $ director : chr &quot;Ryan Coogler&quot; &quot;James Cameron&quot; &quot;Jean-Fran?ois Richet&quot; &quot;Dan Kwan, \\nDaniel Scheinert&quot; ... ## $ star : chr &quot;Letitia Wright, \\nLupita Nyong&#39;o, \\nDanai Gurira, \\nWinston Duke&quot; &quot;Sam Worthington, \\nZoe Saldana, \\nSigourney Weaver, \\nStephen Lang&quot; &quot;Gerard Butler, \\nMike Colter, \\nTony Goldwyn, \\nYoson An&quot; &quot;Michelle Yeoh, \\nStephanie Hsu, \\nJamie Lee Curtis, \\nKe Huy Quan&quot; ... ## $ gross : int NA NA NA NA NA NA NA NA NA NA ... From the data summary table, note that some variables’ data type does not seem to be reasonable, such as rating should be integer type and certificate should be categorical type. I will fix this by assigning appropriate data type manually. Another noteworthy point is that there are many NA values in gross variable, which is our response variable. This inspires me to visualize the missing rate for each column in the dataset and see if other columns also contains a lot of NA values since this may introduce some problems in model fitting part. 2.2.2 Visualizing Missing Values (#fig:Missing Plot)Histogram of Sepal.Width Figure above shows that the missing values in gross variable counts for 93.38% proportion, which is non negligible. And there are also considerably large proportion missing values in year and rating variables. However, since the total number of rows the dataset has is enormous, we still have plenty observations to fit models even after deleting those missing values. For simplifying model fitting, we did some variable transformations. First, since each movie can be classified as multiple genres, we only assign the main genre to genre variable. Second, I used similar procedure to handle director variable and found out that there are total of 5297 unique directors. Since the number of levels are so much, it will spent a lot of time to fit models if I include director variable. Hence, I decide to drop this variable and the same reason applies for dropping actor variable. 2.3 Box Office Vs Genre We want to see how box office is distributed for different genres. From the plot, it seems that the distribution does not vary a lot from genre to genre, i.e., they all have a long right tail and there are many data points on the left hand side . This might indicate that genre is not a good estimator since movie box office cannot be distinguished from genre to genre. But on the other hand, note that there are no observations for comedy, crime, and drama movies with box office greater than 250 millions but we can find some records for action, adventure, and animation movies with box office larger than 250 millions. Thus, genre may could help to explain some variations in gross variable. 2.4 Box Office Vs Rating &amp; Certificate ## Var1 Freq ## 16 R 8315 ## 15 PG-13 4087 ## 14 PG 3493 ## 12 Not Rated 2072 ## 7 G 940 ## 1 809 ## 6 Approved 347 ## 13 Passed 329 ## 22 Unrated 304 ## 19 TV-MA 69 ## 17 TV-14 57 ## 8 GP 55 ## 20 TV-PG 43 ## 4 18+ 29 ## 11 NC-17 19 ## 23 X 17 ## 18 TV-G 14 ## 9 M 13 ## 10 M/PG 11 ## 21 TV-Y7 5 ## 2 13+ 4 ## 3 16+ 1 ## 5 AO 1 This table shows the number of movies for each certificate. There are total of 23 levels, which is relatively large. And note that the total number of counts for levels other than the top five add up to roughly thousand. Hence, it is reasonable to merge those levels and treat as a new level called other. We also draw the scatter plot of gross versus rating and certificate. For the relationship between gross and rating, in general, movies with high rating seems to have wider range of movie box office while movies with low rating tend to have narrower ranger of movie box office and mainly have low value. Besides, linearity seems to be insufficient to explain as we can see that the fitted line (black line) has a very small slope. As for the relationship between gross and certificate, note that for movies with the same rating score, PG and PG-13 movies tend to have more box office while Not Rated movies tend to have less box office. And in terms of the whole data points, they are highly separable by certificate since more green points are on the top layer, more yellow points are on the middle layer, and more black points are on the bottom layer. Thus, certificate might be a good estimator. "],["linear-regression.html", "Chapter 3 Linear Regression 3.1 Model Fitting 3.2 Model Evaluation 3.3 Model Interpretation", " Chapter 3 Linear Regression 3.1 Model Fitting 3.1.1 Model Summary ## ## Call: ## lm(formula = gross ~ year + certificate + runtime + genre + rating, ## data = train_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -290.69 -26.08 -7.30 13.03 830.52 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -137.85487 8.63938 -15.957 &lt; 2e-16 *** ## year1970 -3.87542 9.42478 -0.411 0.680935 ## year1971 -2.80330 9.59515 -0.292 0.770169 ## year1972 -15.06170 9.16267 -1.644 0.100234 ## year1973 -13.95687 9.26134 -1.507 0.131828 ## year1974 -14.56539 9.15107 -1.592 0.111480 ## year1975 -3.12536 10.39778 -0.301 0.763739 ## year1977 3.97240 10.57808 0.376 0.707270 ## year1978 2.84313 10.60215 0.268 0.788575 ## year1979 -0.18021 10.51887 -0.017 0.986332 ## year1980 10.64177 9.70045 1.097 0.272640 ## year1981 5.10760 9.21075 0.555 0.579226 ## year1982 2.59656 9.11590 0.285 0.775771 ## year1983 6.98645 9.09738 0.768 0.442520 ## year1984 2.61209 8.92271 0.293 0.769719 ## year1985 -7.95517 8.70555 -0.914 0.360833 ## year1986 3.19815 8.58474 0.373 0.709496 ## year1987 1.02436 8.51156 0.120 0.904208 ## year1988 5.22470 8.50893 0.614 0.539207 ## year1989 8.44815 8.55863 0.987 0.323612 ## year1990 11.29015 8.54004 1.322 0.186178 ## year1991 10.18008 8.63533 1.179 0.238460 ## year1992 13.53419 8.53408 1.586 0.112781 ## year1993 11.16539 8.55122 1.306 0.191670 ## year1994 9.06280 8.46649 1.070 0.284441 ## year1995 11.86080 8.45463 1.403 0.160672 ## year1996 13.81837 8.45351 1.635 0.102145 ## year1997 11.76002 8.36493 1.406 0.159781 ## year1998 16.74185 8.36450 2.002 0.045351 * ## year1999 25.37426 8.43803 3.007 0.002641 ** ## year2000 16.93140 8.32003 2.035 0.041865 * ## year2001 16.10946 8.32936 1.934 0.053123 . ## year2002 19.23010 8.31165 2.314 0.020700 * ## year2003 19.19027 8.35074 2.298 0.021572 * ## year2004 14.76399 8.29930 1.779 0.075267 . ## year2005 20.32833 8.29115 2.452 0.014224 * ## year2006 15.37888 8.25290 1.863 0.062416 . ## year2007 18.86797 8.25311 2.286 0.022257 * ## year2008 19.56242 8.26219 2.368 0.017910 * ## year2009 18.49899 8.27099 2.237 0.025325 * ## year2010 26.46834 8.28003 3.197 0.001393 ** ## year2011 20.30902 8.26150 2.458 0.013971 * ## year2012 22.47593 8.27189 2.717 0.006592 ** ## year2013 20.29357 8.22216 2.468 0.013591 * ## year2014 18.74780 8.22410 2.280 0.022643 * ## year2015 18.31896 8.21495 2.230 0.025764 * ## year2016 19.67048 8.19812 2.399 0.016433 * ## year2017 24.85789 8.18792 3.036 0.002402 ** ## year2018 17.58427 8.18299 2.149 0.031658 * ## year2019 40.61469 8.41087 4.829 1.39e-06 *** ## year2020 4.74310 10.18953 0.465 0.641588 ## year2021 40.70115 9.51255 4.279 1.89e-05 *** ## yearOther 5.95449 8.00326 0.744 0.456882 ## certificateOther 22.60671 1.88203 12.012 &lt; 2e-16 *** ## certificatePG 58.25183 1.84888 31.507 &lt; 2e-16 *** ## certificatePG-13 61.69036 1.69226 36.454 &lt; 2e-16 *** ## certificateR 25.36004 1.58204 16.030 &lt; 2e-16 *** ## runtime 0.38742 0.02071 18.711 &lt; 2e-16 *** ## genreAdult -10.91955 31.37147 -0.348 0.727790 ## genreAdventure -9.65920 1.71302 -5.639 1.74e-08 *** ## genreAnimation 13.72542 1.81341 7.569 3.96e-14 *** ## genreBiography -38.84403 2.99654 -12.963 &lt; 2e-16 *** ## genreComedy -21.43946 1.29996 -16.492 &lt; 2e-16 *** ## genreCrime -22.64123 1.54289 -14.675 &lt; 2e-16 *** ## genreDrama -29.53421 1.36724 -21.601 &lt; 2e-16 *** ## genreFamily -19.61369 9.28584 -2.112 0.034683 * ## genreFantasy -7.60225 4.68469 -1.623 0.104654 ## genreFilm-Noir -36.21209 24.33484 -1.488 0.136750 ## genreHistory -23.66423 27.16556 -0.871 0.383706 ## genreHorror -0.57666 2.11120 -0.273 0.784748 ## genreMusic -22.32211 54.34872 -0.411 0.681283 ## genreMusical -17.40108 16.40813 -1.061 0.288925 ## genreMystery -24.14321 6.29763 -3.834 0.000127 *** ## genreRomance -37.53941 8.73826 -4.296 1.75e-05 *** ## genreSci-Fi -28.84733 20.54008 -1.404 0.160206 ## genreThriller -13.24588 7.31069 -1.812 0.070027 . ## genreWar -48.76727 54.31289 -0.898 0.369254 ## rating 14.16936 0.45469 31.163 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 54.24 on 16749 degrees of freedom ## Multiple R-squared: 0.2614, Adjusted R-squared: 0.258 ## F-statistic: 76.97 on 77 and 16749 DF, p-value: &lt; 2.2e-16 We can read from the summary of the linear model that the R-squared of our model on the training set is only 0.2614, which indicates a very bad fit. To further examine whether the linearity assumption is hold, we draw the residual plot as following. 3.1.2 Check Linearity From the Residual Plot, we can observe that majority points are lying in a line with negative slope. This might indicate that our model will over-estimate certain types movies’ box office and under-estimate some other types. That is, some common characteristics of data with high fitted value contributes a lot to its over all prediction and vice versa for data with low predicted value. Also note that we have negative fitted value, which obviously does not make sense in reality. By checking the dataset, we have no negative values in gross variable. This excludes the possibility that negative fitted values come from dataset. Hence, this is another factor supporting that a linear model might not be appropriate. 3.2 Model Evaluation ## MSE MAE R-Squared ## 1 2726.577 31.71711 0.2759302 The evaluation table shows the MSE, MAE, and R-Squared value of our model on the test set. All of three criterion indicate that our model had a bad performance. Hence, from the aspect of accuracy, linear model should not be our target model. 3.3 Model Interpretation 3.3.1 Partial Dependence Plot Next, we want to explore the marginal effect of rating, runtime, certificate, and genre on predicted value. Note that the PDP of rating and rutime are both straight line, this is because in linear model, the relationship between response and each feature is linear. We can also see that both variables have positive marginal effect on gross. However, the range of movie box office in two figures are different, rating results in a larger variation in terms of predicted value. This suggests that rating could explain more variation of movie box office. Therefore, rating should be a more important feature compared with runtime. And this is consistent with our intuition. The PDP of certificate shows that PG-13 and PG movies tend to have more box office while Not Rated movies tend to have less box office. This is consistent with the results in scatter plot of box office Vs certificate. The PDP of genre shows that animation, action, and horror movies are top three most popular types while war, film-noir, and biography are the three types with least audience. 3.3.2 Local Interpretable Model-agnostic Explanations (LIME) Because of the high interpretability of linear model, LIME features plots seem to be redundant. But I want to see whether the results obtained from local interpretable model is consistent with the ordinary linear model. model_intercept model_prediction feature feature_value feature_weight feature_desc prediction 93.333 45.399 year 27 -2.133 year = 1996 43.805 93.333 45.399 certificate 3 27.663 certificate = PG 43.805 93.333 45.399 runtime 100 -58.298 runtime &lt;= 232 43.805 93.333 45.399 genre 1 16.098 genre = Action 43.805 93.333 45.399 rating 5 -31.264 3.27 &lt; rating &lt;= 5.25 43.805 From the explanation table, we have that the local model for case 3649 is \\(\\hat{y}_{lime} = 93.333 -31.264 \\cdot \\mathbf{1}_{3.27 &lt; rating &lt;= 5.25}+27.663 \\cdot \\mathbf{1}_{certificate = PG}+16.098 \\cdot \\mathbf{1}_{genre = Action}-58.298 \\cdot \\mathbf{1}_{runtime &lt;= 232}-2.133 \\cdot \\mathbf{1}_{year = 1996}\\). And note that in our original model, the coefficient of certificatePG is 58.252, genreAction is baseline and the coefficient of most levels are negative, the coefficient of year1996 is 13.818, the coefficient of rating is 14.169, and the coefficient of runtime is 0.387. Thus, the local (linear) model is different from our original linear model from quantitative aspect. However, from the LIME plot, in terms of direction, the effect of certificate and genre are both consistent with results from original model. Hence from this respect, the local model has same output as original linear model. ## lime error original error ## 1 -42.78843 -16.20443 ## 2 118.21543 102.77843 ## 3 -28.09811 -26.50411 ## 4 -13.22456 2.63944 The residual table gives the residual of local model and original model on the 4 data points used in LIME. We can see that our ordinary linear model outperforms the local model. I think this may be related to the motivation of using lime, that is, we want to add interpretability of our original model, which in exchange, sacrificed some accuracy. To be more specific, by using lime, we create some fake data points around the data point that we want to predict, and we also used a black box model to estimate those fake points. Hence, those two steps might be the reason why we get an even worse prediction than ordinary linear model does. However, linear model is already highly interpretable, we might only need assistance of lime for less interpretable models. 3.3.2.1 Gower Distance As Gower distance increases, the variance of difference between pairs increases. This indicates that: for very different observations, our model sometimes makes very different predictions and sometimes makes similar predictions; for very similar observations, our model tend to consistently make similar predictions. From the figure, we can claim that for our linear model, if the Gower distance is less than 0.18, the prediction will be very similar (difference&lt;25). 3.3.3 Shapley Additive Explanations For linear model, the shapley value of each feature is merely its coefficient. Hence, we do not need this technique to help interpret the model. 3.3.4 Feature Importance The Feature Importance plot shows the relative importance of each feature fed into our linear model. Surprisingly, the most important feature is certificate instead of rating this is different from conclusion derived from partial dependence plot, that is rating is the most important feature as rating caused the most variation in response variable. "],["decision-tree.html", "Chapter 4 Decision Tree 4.1 Model Visualization 4.2 Model Evaluation 4.3 Model Interpretation", " Chapter 4 Decision Tree 4.1 Model Visualization ## [1] 0.7756755 The plot visualized our regression tree model. Since the size of the tree is so large, visualization does not work here. But we can find out each node’s information and splitting rule by checking the summary of tree model. 4.2 Model Evaluation ## MSE MAE R-Squared ## 1 1567.24 19.46132 0.5838036 The evaluation table shows regression tree’s performance on the test set, comparing with linear model, tree model has much better performance no matter which criterion we use. Hence, in terms of accuracy, decision tree would be my choice. 4.3 Model Interpretation 4.3.1 Partial Dependence Plot Comparing the PDP of rating by using tree model and linear model, we find that the overall trend is the same, i.e., as rating goes up, predicted movie box office rises. And one difference is that, instead of being a straight line in linear model, we can observe that the line in tree model has ups and downs. This is because in tree model, the relationship between rating and gross does not have to be linear. Clearly, the latter is what we observed in reality. More insights that we can derive from this plot is that as rating increases from 4 to 6, movie box office does not increase a lot. This is probably because for those movies, people would watch and rate them online, but are not willing to buy tickets to watch them in theater. Hence, they all tend to have few box office. As for why there is a huge jump when rating increased from 7.9 to 8.3, I find it hard to give a reasonable guess. Again, the consistence and difference between PDP of runtime by using tree model and linear model is similar to what we get at rating part: the overall trend is the same and we can get more insights from tree model. One more thing to notice is that, the range of movie box office in tree model for the two different features are not as much as it in linear model. This suggests that rating and runtime in tree model may contribute relatively equal explanations for variations in response variable. The PDP of certificate in tree model shows that PG and PG-13 movies tend to have more box office while Not Rated movies tend to have less box office. This is similar to the results obtained from linear model. The PDP of genre in tree model shows that action, animation, and horror movies are top three most popular types while drama, biography, and romance are the three types with least audience. In general, this is consistent with the conclusion by using linear model. But note that, the difference between each genre in tree model are far less than the difference in linear model. This indicates that genre in tree model is not as much important as it is in linear model (later by drawing the feature importance plot, we find the genre is least important in tree model while it is the second important in linear model). 4.3.2 Local Interpretable Model-agnostic Explanations (LIME) model_intercept model_prediction feature feature_value feature_weight feature_desc prediction -24.77 111.018 year 27 24.136 year = 1996 8.625 -24.77 111.018 certificate 3 52.221 certificate = PG 8.625 -24.77 111.018 runtime 100 46.472 runtime &lt;= 232 8.625 -24.77 111.018 genre 1 36.028 genre = Action 8.625 -24.77 111.018 rating 5 -23.069 3.27 &lt; rating &lt;= 5.25 8.625 From the explanation table, we have that the local model for case 3649 is \\(\\hat{y}_{lime} = -22.479 -24.341 \\cdot \\mathbf{1}_{3.27 &lt; rating &lt;= 5.25}+61.044 \\cdot \\mathbf{1}_{certificate = PG}+33.229 \\cdot \\mathbf{1}_{genre = Action}+44.626 \\cdot \\mathbf{1}_{runtime &lt;= 232}+41.078 \\cdot \\mathbf{1}_{year = 1996}\\). By comparing this result with lime model get in linear part, we find that even the direction of the effect of some features changed. For example, \\(runtime\\leq232\\) has a negative contribution to prediction in linear part, but it has a positive contribution here. Another difference is that the feature weight also changed a lot, certificate in linear part only rank as the third one but has the largest weight here. Since in linear part, the interpretation given by local model and ordinary linear model are in general consistent, my guess about the difference between two local models is that they are just the representation of the original models, i.e., the linear model and the tree model gives different prediction for the same data points, and this is reflected on difference of local models. ## lime error original error ## 1 -33.19643 -13.082431 ## 2 106.24943 -20.313573 ## 3 -96.67111 8.675889 ## 4 -33.11856 1.751440 The residual table shows that our tree model also outperforms its local model. Also note that the error for each case in tree model is also less than it in linear model. 4.3.2.1 Gower Distance From the plot, we still have that as Gower distance increases, the variance of difference between pairs increases. One major distinction from linear part is that for pairs at the same difference level (the Gower distance is same), tree model tend to make more variant predictions compared with linear model. In my opinion, I think this might be an advantage since considering the features we include, two movies defined as similar will just have common characteristics in terms of rate, run time, genre, certificate, and released year. But for movies with even all the same features mentioned above, in real case, they still tend to have very different box office as many other factors could have impact on, such as season, published area, and published volume in cinema every day. 4.3.3 Shapley Additive Explanations From the SHAP plot, case 10 has the most prediction value, runtime and rating contribute majority of the difference between mean gross and prediction of case 10. By checking the feature value of each case in our sample, we find that case 10 has the highest rate and longest run time. On the other hand, case 4464 has the least prediction value, year is the main factor caused the low prediction. And we can see that case 4464 is the oldest movie which is published in 1973. By comparing the interpretation provided by SHAP and by LIME, they seems to be not consistent with each other for some features. For example, for case 10, LIME implies that runtime has a negative effect on predicted value while SHAP indicates that runtime has a positive effect. In this particular case, it seems that SHAP’s explanation is more plausible since although case 10 has a run time less than 232 minutes, we can check that its run time is 155 minutes which is larger than most movies in our data set. Hence, runtime should make positive contribution for case 10. The question is, in general, SHAP and LIME, which one provides a more reasonable interpretation or does it depend on situation and purpose. 4.3.4 Feature Importance The feature importance plot shows that year is the most important feature while genre is the least important feature. This seems to be consistent with result obtained from partial dependence plot. When compared with feature importance in linear model, they are quiet different. Year in linear model is not as important as it is in tree model. My opinion is that the relationship between year and gross is not linear but will fluctuate, hence linear model could not capture the impact of year. But linear constraint is not a problem for tree model, hence tree model could utilize this feature well. "],["random-forest.html", "Chapter 5 Random Forest 5.1 Model Fitting 5.2 Model Evaluation 5.3 Model Interpretation", " Chapter 5 Random Forest 5.1 Model Fitting ## ## Call: ## randomForest(formula = gross ~ year + certificate + runtime + genre + rating, data = train_dat, importance = TRUE) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 1496.916 ## % Var explained: 62.24 We can read from the information of random forest model that the R-Squared value on the training set is 0.6224, which is less than the R-Squared value of tree model on the training set 0.7757, but is larger than the R-Squared value of linear model on the training set 0.2641. However, considering the tree model tend to be overfitting, I should compare their performance on the test set. 5.2 Model Evaluation ## MSE MAE R-Squared ## 1 1445.054 20.93738 0.6162513 The evaluation table shows random forest’s performance on the test set. Random forest has slightly larger R-Squared value than tree model and much larger R-Squared value than linear model. But since the R-Squared value only increased from 0.5838 to 0.6163, I do not think random forest and tree model has very different performance. 5.3 Model Interpretation 5.3.1 Partial Dependence Plot Again the PDP of rating shows that the overall trend is the same as PDP in linear model and tree model: as rating goes up, predicted movie box office rises. However, we can note that the range of movie box office is much larger than the range of movie box office in linear and tree model. This shows that rating is much more important in random forest than as it is in linear and tree model. And this is what I expected since intuitively, high quality movies will attract more people to see. The PDP of runtime in random forest leads to similar conclusion: movie with longer run time tend to have more box office. But note that the range of box office is almost a half of it in rating part, this suggests that runtime is far less important than rating in random forest model. We did not observe this difference in tree model and we indeed observed a little difference in linear model but it is not obvious as it is in random forest model. In my opinion, I support the conclusion that rating is much more important than runtime since length of a movie is never a deterministic factor of whether it is a good or bad movie, hence less impact on the movie box office. The PDP of certificate in random forest model shows that PG-13 and PG movies tend to have more box office while Not Rated movies tend to have less box office. This is similar to the results obtained from linear and tree model. The PDP of genre in random forest model shows that animation, action, and adventure movies are top three most popular types while drama, biography, and romance are the three types with least audience. This is consistent with the conclusion obtained from linear and tree model. And in terms of the difference between each genre, the plot is more like what we observed in tree model part. 5.3.2 Local Interpretable Model-agnostic Explanations (LIME) model_intercept model_prediction feature feature_value feature_weight feature_desc prediction 13.233 57.499 year 27 -1.574 year = 1996 25.02 13.233 57.499 certificate 3 25.748 certificate = PG 25.02 13.233 57.499 runtime 100 15.070 runtime &lt;= 232 25.02 13.233 57.499 genre 1 17.308 genre = Action 25.02 13.233 57.499 rating 5 -12.285 3.27 &lt; rating &lt;= 5.25 25.02 From the explanation table, we have that the local model for case 3649 is \\(\\hat{y}_{lime} = 77.399 -12.144 \\cdot \\mathbf{1}_{3.27 &lt; rating &lt;= 5.25}+26.044 \\cdot \\mathbf{1}_{certificate = PG}+19.739 \\cdot \\mathbf{1}_{genre = Action}-49.789 \\cdot \\mathbf{1}_{runtime &lt;= 232}+3.814 \\cdot \\mathbf{1}_{year = 1996}\\). By doing comparison with the results from linear and tree model, we find that in terms of direction, linear model and random forest give the same interpretation for case 3649; in terms of magnitude, three models generated pairwise different local models. And we can also find some common place resembled by all three models: runtime = PG and genre = Action has similar scale positive contribution in all three models. ## lime error original error ## 1 -38.81143 -7.771431 ## 2 125.96443 76.427427 ## 3 -47.76211 -8.010111 ## 4 -18.66056 -7.084560 The residual table shows that our random forest model also outperforms its local model. Also note that random forest makes slightly less error than tree model did for case 6155 and case 3649, and make larger error for case 10 and case 4464. This supports the idea that random forest model does not have much better performance than tree model in terms of accuracy. 5.3.2.1 Gower Distance Again we read from the plot that as Gower distance increases, the variance of difference between pairs increases. And note that the range of difference is similar to the range obtained in linear model, and it is about a half of the range get in tree model. As I mentioned in tree model part, I think more variance makes more sense in real situation, thus maybe random forest does not do as well as tree model for this regard. 5.3.3 Shapley Additive Explanations Since case 10 has the most predicted value and case 4464 has the least predicted value in random forest model, we focus on these two cases. From the SHAP plot, runtime and rating contribute majority of the difference between mean gross and prediction of case 10; year is a main factor caused the low prediction for case 4464. This is basically the same as what we derived from tree model. Then, comparing the interpretation provided by SHAP and by LIME, again, they are not consistent with each other for some features. Still taking case 10 as an example, LIME implies that runtime has a negative effect on predicted value while SHAP indicates that runtime has a positive effect. Another point that I want to mention is that if the Shapley value seem to have some randomness, i.e., if I do not fix a seed, every time I run the code, I get different SHAP plot. 5.3.4 Feature Importance The feature importance plot shows that certificate is the most important feature while year is the least important feature. This seems to be not consistent with result obtained from partial dependence plot. For example, from PDP, rating is more important than runtime and certificate as it caused more variation in movie box office. And we also note that the feature importance in three models are different from each other no matter using PDP or some other methods. But in terms of percentage, I think rating’s importance does not change a lot from model to model. "],["reflections.html", "Chapter 6 Reflections 6.1 Takeaways 6.2 Limitation and Future Direction", " Chapter 6 Reflections 6.1 Takeaways After implementing the proposed three models, I compared them from both accuracy and interpretability perspective. As we expected, linear model has the least R-Squared value on test set but is the most interpretable model; decision tree improved the R-Squared value on test set to a large extent while does not sacrifice too much interpretability; random forest model has the largest R-Squared value on test set but does not improve a lot compared with decision tree, and it is the least interpretable model. Considering the trade off between accuracy and interpretability, I might choose decision tree as target model. I also learned how to use some techniques to help to interpret models, such as PDP, LIME, and SHAP. Even for black box model such as random forest, we can get a sense of how it comes up with its prediction by using the above methods. Besides, I also find that different models sometimes will result in conflicting interpretations: such as the direction of the effect of some features. This provides an alternative way to choose model. For example, before fitting models, we may have an expectation for the direction of some features by the common sense, and if we find the model gives opposite result, we may consider to not choose it. Finally, I find that when applying PDP, LIME, and SHAP to a more complex model, I could get more insights compared with what I can infer when doing the same thing but use a simpler model. 6.2 Limitation and Future Direction My data set contains movies from 1930s to 2020s, hence many other factors may influence the movie box office, such as inflation, improvement of life quality, and even the most recent pandemic. Thus, I should do more exploratory analysis in terms of year variable to see if I can find any patterns and then may consider dividing the data set more properly. Besides, I did not do too much on parameter tuning and model selection when fitting models. So maybe I can fit a linear model with better performance by adding some interaction terms or doing some variable transformations. Finally, I only know how to use PDP, LIME, and SHAP, but is not very clear about more theoretical part such as the mathematics algorithm behind those methods. Hence, I could not answer questions like why LIME and SHAP give different interpretations. This could be something that worth to explore more in the future. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
