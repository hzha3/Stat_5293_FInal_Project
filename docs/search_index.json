[["index.html", "Movie Box Office Chapter 1 Proposal 1.1 Data 1.2 Modeling Goal 1.3 Models", " Movie Box Office Hongwei Zha 2023-04-29 Chapter 1 Proposal 1.1 Data The data comes from Kaggle: https://www.kaggle.com/datasets/rajugc/imdb-movies-dataset-based-on-genre. And the original source is IMDB:https://www.imdb.com/interfaces/. For simplicity, I do not intend to use all the data, so I just used data from Kaggle. 1.2 Modeling Goal The main goal would be predicting the box office of a new movie based on its IMDB rating, certificate, genre, run time, year, casts, and directors. Some concerns about the practicability is that casts and directors contain many names, to simplify it, I decide to only include the top three actors and only one main director. Then I will convert them to dummy variables, i.e., each actor or director as a feature. Also note that, what we want to predict is the ultimate gross box office of the movie, so we can collect the new movie’s rating after it is released for certain time. I am also interested in finding whether people prefer old movies or new released movies. For example, I can calculate the mean ratings of movies in each year and see how the trend looks like. 1.3 Models The first model that I intend to use will be the ordinary linear regression model. An issue would be that the underlying relationship is not linear. So I may implement generalized linear model with a Gamma distribution (or I can do some tests to see what kind of exponential distributions that box office is closet to) as the second, decision tree as the third, and random forest as the fourth model. "],["data-pre-processing.html", "Chapter 2 Data pre-processing 2.1 Data Dictionary 2.2 Data Summary 2.3 Box Office Vs Genre 2.4 Box Office Vs Rating &amp; Certificate", " Chapter 2 Data pre-processing 2.1 Data Dictionary movie_id – IMDB Movie ID movie_name – Name of the movie year - Release year certificate – Certificate of the movie run_time – Total movie run time genre – Genre of the movie rating – Rating of the movie description – Description of the movie director – Director of the movie director_id – IMDB id of the director star – Star of the movie star_id – IMDB id of the star votes – Number of votes in IMDB website gross – Gross Box Office of the movie in million 2.2 Data Summary 2.2.1 Data Statistics ## &#39;data.frame&#39;: 318269 obs. of 8 variables: ## $ year : chr &quot;2022&quot; &quot;2022&quot; &quot;2023&quot; &quot;2022&quot; ... ## $ certificate: chr &quot;PG-13&quot; &quot;PG-13&quot; &quot;R&quot; &quot;R&quot; ... ## $ runtime : chr &quot;161 min&quot; &quot;192 min&quot; &quot;107 min&quot; &quot;139 min&quot; ... ## $ genre : chr &quot;Action, Adventure, Drama&quot; &quot;Action, Adventure, Fantasy&quot; &quot;Action, Thriller&quot; &quot;Action, Adventure, Comedy&quot; ... ## $ rating : chr &quot;6,9&quot; &quot;7,8&quot; &quot;6,5&quot; &quot;8&quot; ... ## $ director : chr &quot;Ryan Coogler&quot; &quot;James Cameron&quot; &quot;Jean-Fran?ois Richet&quot; &quot;Dan Kwan, \\nDaniel Scheinert&quot; ... ## $ star : chr &quot;Letitia Wright, \\nLupita Nyong&#39;o, \\nDanai Gurira, \\nWinston Duke&quot; &quot;Sam Worthington, \\nZoe Saldana, \\nSigourney Weaver, \\nStephen Lang&quot; &quot;Gerard Butler, \\nMike Colter, \\nTony Goldwyn, \\nYoson An&quot; &quot;Michelle Yeoh, \\nStephanie Hsu, \\nJamie Lee Curtis, \\nKe Huy Quan&quot; ... ## $ gross : int NA NA NA NA NA NA NA NA NA NA ... From the data summary table, note that some variables’ data type does not seem to be reasonable, such as rating should be integer type and certificate should be categorical type. I will fix this by assigning appropriate data type manually. Another noteworthy point is that there are many NA values in gross variable, which is our response variable. This inspires me to visualize the missing rate for each column in the dataset and see if other columns also contains a lot of NA values since this may introduce some problems in model fitting part. 2.2.2 Visualizing Missing Values (#fig:Missing Plot)Histogram of Sepal.Width Figure above shows that the missing values in gross variable counts for 93.38% proportion, which is non negligible. And there are also considerably large proportion missing values in year and rating variables. However, since the total number of rows the dataset has is enormous, we still have plenty observations to fit models even after deleting those missing values. For simplifying model fitting, we did some variable transformations. First, since each movie can be classified as multiple genres, we only assign the main genre to genre variable. Second, I used similar procedure to handle director variable and found out that there are total of 5297 unique directors. Since the number of levels are so much, it will spent a lot of time to fit models if I include director variable. Hence, I decide to drop this variable and the same reason applies for dropping actor variable. 2.3 Box Office Vs Genre We want to see how box office is distributed for different genres. From the plot, it seems that the distribution does not vary a lot from genre to genre, i.e., they all have a long right tail and there are many data points on the left hand side . This might indicate that genre is not a good estimator since movie box office cannot be distinguished from genre to genre. But on the other hand, note that there are no observations for comedy, crime, and drama movies with box office greater than 250 millions but we can find some records for action, adventure, and animation movies with box office larger than 250 millions. Thus, genre may could help to explain some variations in gross variable. 2.4 Box Office Vs Rating &amp; Certificate ## Var1 Freq ## 16 R 8315 ## 15 PG-13 4087 ## 14 PG 3493 ## 12 Not Rated 2072 ## 7 G 940 ## 1 809 ## 6 Approved 347 ## 13 Passed 329 ## 22 Unrated 304 ## 19 TV-MA 69 ## 17 TV-14 57 ## 8 GP 55 ## 20 TV-PG 43 ## 4 18+ 29 ## 11 NC-17 19 ## 23 X 17 ## 18 TV-G 14 ## 9 M 13 ## 10 M/PG 11 ## 21 TV-Y7 5 ## 2 13+ 4 ## 3 16+ 1 ## 5 AO 1 This table shows the number of movies for each certificate. There are total of 23 levels, which is relatively large. And note that the total number of counts for levels other than the top five add up to roughly thousand. Hence, it is reasonable to merge those levels and treat as a new level called other. We also draw the scatter plot of gross versus rating and certificate. For the relationship between gross and rating, in general, movies with high rating seems to have wider range of movie box office while movies with low rating tend to have narrower ranger of movie box office and mainly have low value. Besides, linearity seems to be insufficient to explain as we can see that the fitted line (black line) has a very small slope. As for the relationship between gross and certificate, note that for movies with the same rating score, PG and PG-13 movies tend to have more box office while Not Rated movies tend to have less box office. And in terms of the whole data points, they are highly separable by certificate since more green points are on the top layer, more yellow points are on the middle layer, and more black points are on the bottom layer. Thus, certificate might be a good estimator. "],["linear-regression.html", "Chapter 3 Linear Regression 3.1 Model Fitting 3.2 Model Evaluation 3.3 Model Interpretation", " Chapter 3 Linear Regression 3.1 Model Fitting 3.1.1 Model Summary ## ## Call: ## lm(formula = gross ~ year + certificate + runtime + genre + rating, ## data = train_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -295.30 -25.91 -7.31 13.12 830.12 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -120.21244 54.35819 -2.211 0.027016 * ## year1914 21.81797 60.66068 0.360 0.719097 ## year1915 19.50822 60.64788 0.322 0.747712 ## year1916 -30.16239 62.64415 -0.481 0.630176 ## year1917 11.70554 61.07554 0.192 0.848013 ## year1918 21.77507 66.46347 0.328 0.743200 ## year1920 -3.09190 66.44300 -0.047 0.962885 ## year1921 -25.14191 57.18855 -0.440 0.660209 ## year1922 7.27642 55.92548 0.130 0.896482 ## year1923 -10.81645 55.82472 -0.194 0.846368 ## year1924 -21.10937 55.25807 -0.382 0.702456 ## year1925 -9.35677 55.46363 -0.169 0.866034 ## year1926 -17.12813 55.20711 -0.310 0.756373 ## year1927 -14.59860 55.12251 -0.265 0.791137 ## year1928 -20.85300 58.59985 -0.356 0.721954 ## year1929 -8.92249 56.48237 -0.158 0.874483 ## year1930 -9.77738 56.03171 -0.174 0.861477 ## year1931 -19.91362 57.16929 -0.348 0.727599 ## year1932 -14.84490 55.58707 -0.267 0.789429 ## year1933 -6.22247 56.03838 -0.111 0.911586 ## year1934 -9.74593 56.90417 -0.171 0.864014 ## year1935 -21.81367 57.54411 -0.379 0.704635 ## year1936 -11.77056 56.90158 -0.207 0.836123 ## year1937 42.44672 56.15101 0.756 0.449697 ## year1938 -30.04404 57.53829 -0.522 0.601568 ## year1939 -17.72523 54.93811 -0.323 0.746972 ## year1940 -17.64602 55.32981 -0.319 0.749788 ## year1941 -21.64091 55.52255 -0.390 0.696713 ## year1942 -14.71033 55.09430 -0.267 0.789470 ## year1943 -3.81440 56.67667 -0.067 0.946343 ## year1944 -17.06934 56.13977 -0.304 0.761093 ## year1945 -4.54327 57.96989 -0.078 0.937532 ## year1946 -14.23288 55.45769 -0.257 0.797457 ## year1947 -9.36129 55.65378 -0.168 0.866423 ## year1948 -16.83511 55.83187 -0.302 0.763012 ## year1949 -16.58324 57.18432 -0.290 0.771823 ## year1950 1.86317 55.91450 0.033 0.973418 ## year1951 -29.10629 55.93679 -0.520 0.602832 ## year1952 -19.86908 55.91867 -0.355 0.722354 ## year1953 9.81371 55.73363 0.176 0.860231 ## year1954 -15.11584 54.78137 -0.276 0.782605 ## year1955 -11.75886 55.47774 -0.212 0.832144 ## year1956 -25.74408 55.73971 -0.462 0.644186 ## year1957 -12.35089 55.31734 -0.223 0.823326 ## year1958 -26.83331 55.65458 -0.482 0.629713 ## year1959 -14.21580 54.99793 -0.258 0.796041 ## year1960 -11.74289 54.98280 -0.214 0.830882 ## year1961 1.83519 55.32606 0.033 0.973539 ## year1962 -33.82941 55.25549 -0.612 0.540390 ## year1963 -19.24221 54.81366 -0.351 0.725557 ## year1964 -18.20523 54.85056 -0.332 0.739964 ## year1965 -7.93468 54.92975 -0.144 0.885146 ## year1966 -21.65961 55.58975 -0.390 0.696813 ## year1967 -5.93036 55.04621 -0.108 0.914208 ## year1968 -15.06788 55.14686 -0.273 0.784678 ## year1969 -28.13156 54.80169 -0.513 0.607725 ## year1970 -23.10140 54.50019 -0.424 0.671661 ## year1971 -22.02269 54.53256 -0.404 0.686332 ## year1972 -34.29530 54.45863 -0.630 0.528867 ## year1973 -33.20227 54.47727 -0.609 0.542221 ## year1974 -33.85036 54.45950 -0.622 0.534233 ## year1975 -22.38263 54.68181 -0.409 0.682307 ## year1976 -22.67878 54.88289 -0.413 0.679450 ## year1977 -15.28429 54.72027 -0.279 0.780005 ## year1978 -16.43556 54.72380 -0.300 0.763924 ## year1979 -19.47260 54.70876 -0.356 0.721896 ## year1980 -8.58997 54.55490 -0.157 0.874888 ## year1981 -14.10976 54.46931 -0.259 0.795607 ## year1982 -16.56371 54.45440 -0.304 0.760998 ## year1983 -12.19302 54.44914 -0.224 0.822811 ## year1984 -16.59718 54.42273 -0.305 0.760395 ## year1985 -27.08100 54.38909 -0.498 0.618552 ## year1986 -15.88751 54.36573 -0.292 0.770111 ## year1987 -18.10011 54.35394 -0.333 0.739135 ## year1988 -13.90428 54.35490 -0.256 0.798104 ## year1989 -10.66204 54.36355 -0.196 0.844515 ## year1990 -7.82602 54.36007 -0.144 0.885529 ## year1991 -8.94508 54.37569 -0.165 0.869335 ## year1992 -5.59718 54.35718 -0.103 0.917988 ## year1993 -7.95428 54.36160 -0.146 0.883669 ## year1994 -10.07551 54.34893 -0.185 0.852929 ## year1995 -7.27961 54.34619 -0.134 0.893445 ## year1996 -5.30318 54.34546 -0.098 0.922265 ## year1997 -7.37382 54.33127 -0.136 0.892045 ## year1998 -2.35611 54.33159 -0.043 0.965411 ## year1999 6.25248 54.34254 0.115 0.908401 ## year2000 -2.17594 54.32491 -0.040 0.968050 ## year2001 -3.01000 54.32861 -0.055 0.955818 ## year2002 0.13465 54.32269 0.002 0.998022 ## year2003 0.07259 54.33125 0.001 0.998934 ## year2004 -4.32085 54.32424 -0.080 0.936606 ## year2005 1.23536 54.32180 0.023 0.981857 ## year2006 -3.68250 54.31620 -0.068 0.945948 ## year2007 -0.21512 54.31481 -0.004 0.996840 ## year2008 0.49141 54.31782 0.009 0.992782 ## year2009 -0.57034 54.32058 -0.010 0.991623 ## year2010 7.38048 54.32206 0.136 0.891929 ## year2011 1.26108 54.32096 0.023 0.981479 ## year2012 3.46848 54.32303 0.064 0.949091 ## year2013 1.23753 54.31414 0.023 0.981822 ## year2014 -0.33808 54.31343 -0.006 0.995034 ## year2015 -0.72149 54.31382 -0.013 0.989402 ## year2016 0.62123 54.30969 0.011 0.990874 ## year2017 5.80182 54.30660 0.107 0.914922 ## year2018 -1.51075 54.30758 -0.028 0.977807 ## year2019 21.53249 54.34443 0.396 0.691946 ## year2020 -14.34845 54.64520 -0.263 0.792882 ## year2021 21.55288 54.52547 0.395 0.692641 ## year2022 40.65133 56.04659 0.725 0.468270 ## certificateOther 23.17051 1.90747 12.147 &lt; 2e-16 *** ## certificatePG 58.77324 1.86031 31.593 &lt; 2e-16 *** ## certificatePG-13 61.97718 1.70025 36.452 &lt; 2e-16 *** ## certificateR 25.75499 1.59208 16.177 &lt; 2e-16 *** ## runtime 0.39365 0.02105 18.702 &lt; 2e-16 *** ## genreAdult -10.98507 31.36537 -0.350 0.726171 ## genreAdventure -9.61763 1.72239 -5.584 2.39e-08 *** ## genreAnimation 13.25524 1.82569 7.260 4.03e-13 *** ## genreBiography -39.29507 3.00102 -13.094 &lt; 2e-16 *** ## genreComedy -21.53476 1.30407 -16.514 &lt; 2e-16 *** ## genreCrime -22.68721 1.54971 -14.640 &lt; 2e-16 *** ## genreDrama -29.68656 1.37094 -21.654 &lt; 2e-16 *** ## genreFamily -19.68815 9.32770 -2.111 0.034811 * ## genreFantasy -7.32926 4.68987 -1.563 0.118122 ## genreFilm-Noir -40.39814 28.60395 -1.412 0.157872 ## genreHistory -27.70580 28.28755 -0.979 0.327380 ## genreHorror -0.46708 2.11403 -0.221 0.825141 ## genreMusic -22.36235 54.33817 -0.412 0.680682 ## genreMusical -15.12664 16.64664 -0.909 0.363527 ## genreMystery -24.22361 6.30639 -3.841 0.000123 *** ## genreRomance -38.43141 8.78622 -4.374 1.23e-05 *** ## genreSci-Fi -27.53180 20.56941 -1.338 0.180757 ## genreThriller -13.33323 7.31328 -1.823 0.068298 . ## genreWar -48.98467 54.30228 -0.902 0.367031 ## rating 14.25298 0.45707 31.184 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 54.23 on 16693 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2583 ## F-statistic: 45.05 on 133 and 16693 DF, p-value: &lt; 2.2e-16 We can read from the summary of the linear model that the R-squared of our model on the training set is only 0.2641, which indicates a very bad fit. To further examine whether the linearity assumption is hold, we draw the residual plot as following. 3.1.2 Check Linearity From the Residual Plot, we can observe that majority points are lying in a line with negative slope. This might indicate that our model will over-estimate certain types movies’ box office and under-estimate some other types. That is, some common characteristics of data with high fitted value contributes a lot to its over all prediction and vice versa for data with low predicted value. Also note that we have negative fitted value, which obviously does not make sense in reality. By checking the dataset, we have no negative values in gross variable. This excludes the possibility that negative fitted values come from dataset. Hence, this is another factor supporting that a linear model might not be appropriate. 3.2 Model Evaluation ## MSE MAE R-Squared ## 1 2725.825 31.79679 0.2761298 The evaluation table shows the MSE, MAE, and R-Squared value of our model on the test set. All of three criterion indicate that our model had a bad performance. Hence, from the aspect of accuracy, linear model should not be our target model. 3.3 Model Interpretation 3.3.1 Partial Dependence Plot Next, we want to explore the marginal effect of rating, runtime, certificate, and genre on predicted value. Note that the PDP of rating and rutime are both straight line, this is because in linear model, the relationship between response and each feature is linear. We can also see that both variables have positive marginal effect on gross. However, the range of movie box office in two figures are different, rating results in a larger variation in terms of predicted value. This suggests that rating could explain more variation of movie box office. Therefore, rating should be a more important feature compared with runtime. And this is consistent with our intuition. The PDP of certificate shows that PG-13 and PG movies tend to have more box office while Not Rated movies tend to have less box office. This is consistent with the results in scatter plot of box office Vs certificate. The PDP of genre shows that animation, action, and horror movies are top three most popular types while war, film-noir, and biography are the three types with least audience. 3.3.2 Local Interpretable Model-agnostic Explanations (LIME) Because of the high interpretability of linear model, LIME features plots seem to be redundant. But I want to see whether the results obtained from local interpretable model is consistent with the ordinary linear model. model_intercept model_prediction feature feature_value feature_weight feature_desc prediction 95.999 49.821 year 92 1.893 year = 1996 43.888 95.999 49.821 certificate 3 28.437 certificate = PG 43.888 95.999 49.821 runtime 100 -60.749 runtime &lt;= 232 43.888 95.999 49.821 genre 1 15.956 genre = Action 43.888 95.999 49.821 rating 5 -31.714 3.27 &lt; rating &lt;= 5.25 43.888 From the explanation table, we have that the local model for case 3649 is \\(\\hat{y}_{lime} = 95.999 -31.714 \\cdot \\mathbf{1}_{3.27 &lt; rating &lt;= 5.25}+28.437 \\cdot \\mathbf{1}_{certificate = PG}+15.956 \\cdot \\mathbf{1}_{genre = Action}-60.749 \\cdot \\mathbf{1}_{runtime &lt;= 232}+1.893 \\cdot \\mathbf{1}_{year = 1996}\\). And note that in our original model, the coefficient of certificatePG is 58.77324, genreAction is baseline and the coefficient of most levels are negative, the coefficient of year1996 is -5.303, the coefficient of rating is 14.253, and the coefficient of runtime is 0.394. Thus, the local (linear) model is different from our original linear model from quantitative aspect. However, from the LIME plot, in terms of direction, the effect of certificate, rating, and genre are both consistent with results from original model. Hence from this respect, the local model has same output as original linear model. ## lime error original error ## 1 -44.65243 -15.96443 ## 2 119.05043 102.17243 ## 3 -32.52011 -26.58711 ## 4 -12.21556 2.65644 The residual table gives the residual of local model and original model on the 4 data points used in LIME. We can see that our ordinary linear model outperforms the local model. I think this may be related to the motivation of using lime, that is, we want to add interpretability of our original model, which in exchange, sacrificed some accuracy. To be more specific, by using lime, we create some fake data points around the data point that we want to predict, and we also used a black box model to estimate those fake points. Hence, those two steps might be the reason why we get an even worse prediction than ordinary linear model does. However, linear model is already highly interpretable, we might only need assistance of lime for less interpretable models. 3.3.2.1 Gower Distance As Gower distance increases, the variance of difference between pairs increases. This indicates that: for very different observations, our model sometimes makes very different predictions and sometimes makes similar predictions; for very similar observations, our model tend to consistently make similar predictions. From the figure, we can claim that for our linear model, if the Gower distance is less than 0.18, the prediction will be very similar (difference&lt;25). 3.3.3 Shapley Additive Explanations For linear model, the shapley value of each feature is merely its coefficient. Hence, we do not need this technique to help interpret the model. 3.3.4 Feature Importance The Feature Importance plot shows the relative importance of each feature fed into our linear model. Surprisingly, the most important feature is certificate instead of rating this is different from conclusion derived from partial dependence plot, that is rating is the most important feature as rating caused the most variation in response variable. "],["decision-tree.html", "Chapter 4 Decision Tree 4.1 Model Fitting 4.2 Model Visualization 4.3 Model Evaluation 4.4 Model Interpretation", " Chapter 4 Decision Tree 4.1 Model Fitting 4.2 Model Visualization The plot visualized our regression tree model. Since the size of the tree is so large, visualization does not work here. But we can find out each node’s information and splitting rule by checking the summary of tree model. 4.3 Model Evaluation ## MSE MAE R-Squared ## 1 1521.118 18.94823 0.5960518 The evaluation table shows regression tree’s performance on the test set, comparing with linear model, tree model has much better performance no matter which criterion we use. Hence, in terms of accuracy, decision tree would be my choice. 4.4 Model Interpretation 4.4.1 Partial Dependence Plot Comparing the PDP of rating by using tree model and linear model, we find that the overall trend is the same, i.e., as rating goes up, predicted movie box office rises. And one difference is that, instead of being a straight line in linear model, we can observe that the line in tree model has ups and downs. This is because in tree model, the relationship between rating and gross does not have to be linear. Clearly, the latter is what we observed in reality. More insights that we can derive from this plot is that as rating increases from 4 to 6, movie box office does not increase a lot. This is probably because for those movies, people would watch and rate them online, but are not willing to buy tickets to watch them in theater. Hence, they all tend to have few box office. As for why there is a huge jump when rating increased from 7.9 to 8.3, I find it hard to give a reasonable guess. Again, the consistence and difference between PDP of runtime by using tree model and linear model is similar to what we get at rating part: the overall trend is the same and we can get more insights from tree model. One more thing to notice is that, the range of movie box office in tree model for the two different features are not as much as it in linear model. This suggests that rating and runtime in tree model may contribute relatively equal explanations for variations in response variable. The PDP of certificate in tree model shows that PG and PG-13 movies tend to have more box office while Not Rated movies tend to have less box office. This is similar to the results obtained from linear model. The PDP of genre in tree model shows that action, animation, and horror movies are top three most popular types while drama, biography, and romance are the three types with least audience. In general, this is consistent with the conclusion by using linear model. But note that, the difference between each genre in tree model are far less than the difference in linear model. This indicates that genre in tree model is not as much important as it is in linear model (later by drawing the feature importance plot, we find the genre is least important in tree model while it is the second important in linear model). 4.4.2 Local Interpretable Model-agnostic Explanations (LIME) model_intercept model_prediction feature feature_value feature_weight feature_desc prediction -22.479 133.157 year 92 41.078 year = 1996 8.625 -22.479 133.157 certificate 3 61.044 certificate = PG 8.625 -22.479 133.157 runtime 100 44.626 runtime &lt;= 232 8.625 -22.479 133.157 genre 1 33.229 genre = Action 8.625 -22.479 133.157 rating 5 -24.341 3.27 &lt; rating &lt;= 5.25 8.625 From the explanation table, we have that the local model for case 3649 is \\(\\hat{y}_{lime} = -22.479 -24.341 \\cdot \\mathbf{1}_{3.27 &lt; rating &lt;= 5.25}+61.044 \\cdot \\mathbf{1}_{certificate = PG}+33.229 \\cdot \\mathbf{1}_{genre = Action}+44.626 \\cdot \\mathbf{1}_{runtime &lt;= 232}+41.078 \\cdot \\mathbf{1}_{year = 1996}\\). By comparing this result with lime model get in linear part, we find that even the direction of the effect of some features changed. For example, \\(runtime\\leq232\\) has a negative contribution to prediction in linear part, but it has a positive contribution here. Another difference is that the feature weight also changed a lot, certificate in linear part only rank as the third one but has the largest weight here. Since in linear part, the interpretation given by local model and ordinary linear model are in general consistent, my guess about the difference between two local models is that they are just the representation of the original models, i.e., the linear model and the tree model gives different prediction for the same data points, and this is reflected on difference of local models. ## lime error original error ## 1 -33.19643 -13.082431 ## 2 106.24943 -20.313573 ## 3 -96.67111 8.675889 ## 4 -33.11856 1.751440 The residual table shows that our tree model also outperforms the local model. Also note that the error for each case in tree model is also less than it in linear model. 4.4.2.1 Gower Distance From the plot, we still have that as Gower distance increases, the variance of difference between pairs increases. One major distinction from linear part is that for pairs at the same difference level (the Gower distance is same), tree model tend to make more variant predictions compared with linear model. In my opinion, I think this might be an advantage since considering the features we include, two movies defined as similar will just have common characteristics in terms of rate, run time, genre, certificate, and released year. But for movies with even all the same features mentioned above, in real case, they still tend to have very different box office as many other factors could have impact on, such as season, published area, and published volume in cinema every day. 4.4.3 Shapley Additive Explanations From the SHAP plot, case 10 has the most prediction value, runtime and rating contribute majority of the difference between mean gross and prediction of case 10. By checking the feature value of each case in our sample, we find that case 10 has the highest rate and longest run time. On the other hand, case 4464 has the least prediction value, year is the main factor caused the low prediction. And we can see that case 4464 is the oldest movie which is published in 1973. By comparing the interpretation provided by SHAP and by LIME, they seems to be not consistent with each other for some features. For example, for case 10, LIME implies that runtime has a negative effect on predicted value while SHAP indicates that runtime has a positive effect. In this particular case, it seems that SHAP’s explanation is more plausible since although case 10 has a run time less than 232 minutes, we can check that its run time is 155 minutes which is larger than most movies in our data set. Hence, runtime should make positive contribution for case 10. The question is, in general, SHAP and LIME, which one provides a more reasonable interpretation or does it depend on situation and purpose. 4.4.4 Feature Importance The feature importance plot shows that year is the most important feature while genre is the least important feature. This seems to be consistent with result obtained from partial dependence plot. When compared with feature importance in linear model, they are quiet different. Year in linear model is not as important as it is in tree model. My opinion is that the relationship between year and gross is not linear but will fluctuate, hence linear model could not capture the impact of year. But linear constraint is not a problem for tree model, hence tree model could utilize this feature well. "],["random-forest.html", "Chapter 5 Random Forest", " Chapter 5 Random Forest "],["reflections.html", "Chapter 6 Reflections", " Chapter 6 Reflections "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
