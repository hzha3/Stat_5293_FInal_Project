# Linear Regression

```{r}
library(broom)
library(pdp)
library(dplyr)
library(lime)
library(relaimpo)
```

## Model Fitting

### Model Summary
```{r}
set.seed(5293)
n = nrow(movie)

train_index = sample(n, 0.8*n)

train_dat = movie[train_index, ]
test_dat = movie[-train_index, ]

lr = lm(gross~year+certificate+runtime+genre+rating, data = train_dat)
summary(lr)
```

We can read from the summary of the linear model that the R-squared of our model on the training set is only 0.2641, which indicates a very bad fit. To further examine whether the linearity assumption is hold, we draw the residual plot as following. 

### Check Linearity

```{r, fig.width=6, fig.height=4}
augmod = augment(lr)
ggplot(augmod, aes(.fitted, .resid)) +
  geom_point(alpha = 0.4, size = 1, pch = 1) +
  geom_hline(yintercept = 0) +
  xlim(NA, 200) +
  ylim(-200, 400) +
  labs(title = 'Residual Plot')
```

From the Residual Plot, we can observe that majority points are lying in a line with negative slope. This might indicate that our model will over-estimate certain types movies' box office and under-estimate some other types. That is, some common characteristics of data with high fitted value contributes a lot to its over all prediction and vice versa for data with low predicted value.  
Also note that we have negative fitted value, which obviously
does not make sense in reality. By checking the dataset, we have no negative values in `gross` variable. This excludes the possibility that negative fitted values come from dataset. Hence, this is another factor supporting that a linear model might not be appropriate. 

## Model Evaluation

```{r}
mse_vec =  function(truth, estimate){
  mean((truth-estimate)^2)
}

mae_vec = function(truth, estimate){
  mean(abs(truth-estimate))
}

rsq_vec = function(truth, estimate){
  1 - sum((truth-estimate)^2) /
    sum((truth - mean(truth))^2)
}

eval_recap = function(truth, estimate){
  df_new = data.frame(truth = truth,
                       estimate = estimate)
  
  data.frame(MSE = mse_vec(truth, estimate),
             MAE = mae_vec(truth, estimate),
             "R-Squared" = rsq_vec(truth, estimate),
             check.names = F
             )
}

eval_recap(test_dat$gross, predict(lr, newdata = test_dat))
```

The evaluation table shows the MSE, MAE, and R-Squared value of our model on the test set. All of three criterion indicate that our model had a bad performance. Hence, from the aspect of accuracy, linear model should not be our target model. 

## Model Interpretation

### Partial Dependence Plot

Next, we want to explore the marginal effect of `rating`, `runtime`, `certificate`, and `genre` on predicted value. 

```{r}
pdp_movie1 = pdp::partial(lr, pred.var = "rating", trim.outliers = TRUE)

ggplot(pdp_movie1, aes(rating, yhat)) +
  geom_line(lwd = 2)+
  geom_rug(data = train_dat |> filter(rating < max(pdp_movie1$rating) & rating > min(pdp_movie1$rating)), aes(rating), inherit.aes = FALSE, alpha = .5, color = "black") +
  #geom_smooth(data = movie |> filter(rating < max(pdp_movie1$rating) & rating > min(pdp_movie1$rating)), aes(rating, gross), se = FALSE) +
  #geom_point(data = movie |> filter(rating < max(pdp_movie1$rating) & rating > min(pdp_movie1$rating)), aes(rating, gross), size = .75, color = "blue", alpha = .25)+
  ylab("movie box office") +
  theme_bw(16)
```

```{r}
pdp_movie2 = pdp::partial(lr, pred.var = "runtime",trim.outliers = TRUE)

ggplot(pdp_movie2, aes(runtime, yhat)) +
  geom_line(lwd = 2)+
  geom_rug(data = train_dat |> filter(runtime < max(pdp_movie2$runtime) & runtime > min(pdp_movie2$runtime)), aes(runtime), inherit.aes = FALSE, alpha = .5, color = "black")+
  #geom_smooth(data = movie |> filter(runtime < max(pdp_movie2$runtime) & runtime > min(pdp_movie2$runtime)), aes(runtime, gross), se = FALSE) +
  #geom_point(data = movie |> filter(runtime < max(pdp_movie2$runtime) & runtime > min(pdp_movie2$runtime)), aes(runtime, gross), size = .75, color = "blue", alpha = .25)+
  ylab("movie box office") +
  theme_bw(16)
```

Note that the PDP of `rating` and `rutime` are both straight line, this is because in linear model, the relationship between response and each feature is linear. We can also see that both variables have positive marginal effect on `gross`. However, the range of movie box office in two figures are different, `rating` results in a larger variation in terms of predicted value. This suggests that `rating` could explain more variation of movie box office. Therefore, `rating` should be a more important feature compared with `runtime`. And this is consistent with our intuition. 

```{r}
pdp_movie3 = pdp::partial(lr, pred.var = "certificate")

ggplot(pdp_movie3, aes(fct_reorder(certificate, yhat, .desc = TRUE), yhat)) +
  geom_col(fill = "cornflowerblue")+
  ylab("movie box office") +
  xlab("certificate")+
  theme_bw(16)
```

The PDP of certificate shows that PG-13 and PG movies tend to have more box office while Not Rated movies tend to have less box office. This is consistent with the results in scatter plot of box office Vs certificate.

```{r}
pdp_movie4 = pdp::partial(lr, pred.var = "genre")

ggplot(pdp_movie4, aes(y = fct_reorder(genre, yhat, .desc = TRUE), x = yhat)) +
  geom_col(fill = "cornflowerblue")+
  xlab("movie box office") +
  ylab("genre")+
  theme_bw(16)
```

The PDP of genre shows that animation, action, and horror movies are top three most popular types while war, film-noir, and biography are the three types with least audience.

### Local Interpretable Model-agnostic Explanations (LIME)

Because of the high interpretability of linear model, LIME features plots seem to be redundant. But I want to see whether the results obtained from local interpretable model is consistent with the ordinary linear model. 

```{r}
set.seed(5293)
n = nrow(movie)
test_index = sample(n, 4)
test_dat2 = movie[test_index,] |> dplyr::select(c("year","certificate","runtime","genre","rating","gross"))
train_dat2 = movie[-test_index,] |> dplyr::select(c("year","certificate","runtime","genre","rating","gross"))

lr2 = lm(gross~year+certificate+runtime+genre+rating, data = train_dat)

predict_model.lm = function(x, newdata, ...) {
  data.frame(predict(x, newdata = newdata))
}

model_type.lm = function(x, ...) {
  return("regression")
}

explainer = lime(train_dat2, lr2, bin_continuous = TRUE, quantile_bins = FALSE)
explanation = lime::explain(test_dat2|>dplyr::select(-gross), explainer, n_features = 5)

knitr::kable(explanation |> filter(case == "3649") |> dplyr::select(c("model_intercept","model_prediction","feature","feature_value","feature_weight","feature_desc","prediction")), digits = 3)

plot_features(explanation)
```

From the explanation table, we have that the local model for case 3649 is $\hat{y}_{lime} = 95.999 -31.714 \cdot \mathbf{1}_{3.27 < rating <= 5.25}+28.437 \cdot \mathbf{1}_{certificate = PG}+15.956 \cdot \mathbf{1}_{genre = Action}-60.749 \cdot \mathbf{1}_{runtime <= 232}+1.893 \cdot \mathbf{1}_{year = 1996}$. And note that in our original model, the coefficient of `certificatePG` is 58.77324, `genreAction` is baseline and the coefficient of most levels are negative, the coefficient of `year1996` is -5.303, the coefficient of `rating` is 14.253, and the coefficient of `runtime` is 0.394. Thus, the local (linear) model is different from our original linear model from quantitative aspect. However, from the LIME plot, in terms of direction, the effect of `certificate`, `rating`, and `genre` are both consistent with results from original model. Hence from this respect, the local model has same output as original linear model. 

```{r}
df = data.frame(cbind(test_dat2$gross-c(64.004,68.655,49.821,15.394), test_dat2$gross-c(35.316,85.533,43.888,0.522)))
colnames(df) = c("lime error", "original error")
df
```

The residual table gives the residual of local model and original model on the 4 data points used in LIME. We can see that our ordinary linear model outperforms the local model. I think this may be related to the motivation of using lime, that is, we want to add interpretability of our original model, which in exchange, sacrificed some accuracy. To be more specific, by using lime, we create some fake data points around the data point that we want to predict, and we also used a black box model to estimate those fake points. Hence, those two steps might be the reason why we get an even worse prediction than ordinary linear model does. However, linear model is already highly interpretable, we might only need assistance of lime for less interpretable models.  

#### Gower Distance

```{r}
set.seed(5293)
n = nrow(movie)
test_index = sample(n, 100)
test_dat3 = movie[test_index,] |> dplyr::select(c("year","certificate","runtime","genre","rating","gross"))
train_dat3 = movie[-test_index,] |> dplyr::select(c("year","certificate","runtime","genre","rating","gross"))

lr3 = lm(gross~year+certificate+runtime+genre+rating, data = train_dat)

df = test_dat3 |>
  cluster::daisy() |>
  as.matrix() |>
  as.data.frame() |>
  rownames_to_column("obs1") |>
  mutate(pred1 = predict(lr3, movie[obs1,])) |>
  pivot_longer(-c(obs1, pred1), names_to = "obs2", values_to = "gower") |>
  filter(obs1 < obs2) |> 
  mutate(pred2 = predict(lr3, movie[obs2,])) |>
  dplyr::select(obs1, obs2, gower, pred1, pred2) |>
  mutate(diff = abs(pred2 - pred1)) |>
  mutate(total = pred1 + pred2)

ggplot(df, aes(gower, diff)) + geom_point(alpha = 0.8, size = 1) +
  ggtitle("Difference in prediction vs. Gower distance")
```

As Gower distance increases, the variance of difference between pairs increases. This indicates that: for very different observations, our model sometimes makes very different predictions and sometimes makes similar predictions; for very similar observations, our model tend to consistently make similar predictions. From the figure, we can claim that for our linear model, if the Gower distance is less than 0.18, the prediction will be very similar (difference<25).

### Shapley Additive Explanations

For linear model, the shapley value of each feature is merely its coefficient. Hence, we do not need this technique to help interpret the model. 

### Feature Importance

```{r}
relImportance = calc.relimp(lr, type = "lmg", rela = TRUE) 
df = data.frame(sort(relImportance$lmg, decreasing=TRUE))
colnames(df) = "importance"
df = df|>
  rownames_to_column("feature")
ggplot(df, aes(x = importance, y = fct_reorder(feature, importance, .desc = TRUE)))+
  geom_col(fill = "cornflowerblue")+
  labs(title = "Feature Importance Plot")+
  ylab("feature")
```

The Feature Importance plot shows the relative importance of each feature fed into our linear model. Surprisingly, the most important feature is `certificate` instead of `rating` this is different from conclusion derived from partial dependence plot, that is `rating` is the most important feature as `rating` caused the most variation in response variable. 